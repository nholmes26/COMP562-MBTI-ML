\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% Short headings should be running head and authors last names

\ShortHeadings{Predicting Personality from Tweets using Machine Learning}{}
\firstpageno{1}

\begin{document}

\title{Predicting Personality from Tweets using Machine Learning}

\author{\name Nathan Holmes \email nholmes@email.unc.edu \\
       \addr University of North Carolina at Chapel Hill\\
       \AND
       \name Namita Krishna \email EMAIL HERE \\
       \addr University of North Carolina at Chapel Hill\\
       \AND
       \name Andrew Mu \email EMAIL HERE \\
       \addr University of North Carolina at Chapel Hill\\
       \AND
       \name Nabeel Rahman \email EMAIL HERE \\
       \addr University of North Carolina at Chapel Hill\\
       }

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper explores the use of machine learning in analyzing social media data, specifically Tweets, to accurately identify Myers-Briggs Type Indicator (MBTI) personality types. Leveraging diverse data encompassing both qualitative and quantitative dimensions, our study employs K-Nearest Neighbors, Logistic Regression, Multinomial Naive Bayes, and Random Forest models to discern nuanced patterns. Through rigorous evaluation, we contribute insights into the effectiveness of these models for personality type identification. This research advances the understanding of the interplay between language use on social media and personality traits, with practical implications for social sciences and human behavior analysis.
\end{abstract}

\begin{keywords}
  MBTI, KNN, Logistic Regression, Multinomial Naive Bayes, Random Forest
\end{keywords}

\section{Introduction}
Classification is a fundamental domain in realm of machine learning research, permeating our daily lives through advanced models that process data with refined sophistication. From discerning spam emails to categorizing patient diagnoses, we are constantly immersed in the practical applications of these advanced technologies. This project delves into the realm of social media, leveraging a diverse dataset of Tweets that encapsulate both qualitative and quantitative dimensions. Within this paper, we undertake the construction and evaluation of accuracy in identifying MBTI types based on users' tweet content. Our approach involves employing a K-Next Neighbor classifier model, a Logistic Regression model, a Multinomial Naive Bayes model, and a Random Forest classifier model to discern nuanced patterns and trends.\\

\section{Data Collection and Model Development}
Our dataset, obtained from Kaggle, consists of a collection of Tweets paired with their corresponding user's MBTI types. This provided us with diverse and invaluable data for identifing patterns between tweet content and a user's personality. After downloading the dataset, we employed preprocessing techniques, including tokenization and vectorization using tools like CountVectorizer. Subsequently, we split the dataset into training and testing sets, facilitating the development of various classification models. The K-Nearest Neighbors classifier, Logistic Regression, Multinomial Naive Bayes, and Random Forest classifier were trained and evaluated to capture nuanced patterns in the data, and each underwent fine-tuning of hyperparameters. Accuracy metrics and confusion matrices were then generated, providing a comprehensive assessment of each model's performance in predicting personality types from social media content.


\section{Evaluating Model Results}
To assess the performance of our classification models, we employ key metrics including accuracy, precision, recall, and F1-score.

The accuracy of a model is determined by the ratio of correctly predicted instances to the total number of instances:

\[ \text{Accuracy} = \frac{\text{TP + TN}}{\text{TP + TN + FP + FN}} \


Additionally, we generate confusion matrices, visual representations of a model's performance, to provide insights into the number of true positive, true negative, false positive, and false negative predictions for each class. This allows us to see which MBTI types we are identifying correctly, and which ones we are confusing. We also take this a step further, and with each model calculate the number of incorrectly predicted MBTI types were only 1 trait, or letter, away from being correct.

\subsection{Multinomial Naive Bayes}
The Multinomial Naive Bayes (MNB) model, well-suited for text classification tasks, emerged as a fitting choice for our Twitter dataset. Rooted in Bayes' theorem, the probability estimation equation for MNB forms the core of our implementation:

\[ P(\textbf{x}|y, \boldsymbol{\theta}) = \prod_{i=1}^{n} \frac{N_{yi} + \alpha}{N_{y} + \alpha \cdot N_{\text{total}}} \]

Here, the probability of a given tweet belonging to a personality class given model parameters, \(P(\textbf{x}|y, \boldsymbol{\theta})\), is equal to the product (\(\prod_{i=1}^{n}\)) of all possible probability estimates of a word being in tweets by a specific MBTI type (\(N_{yi} + \alpha\)) out of the total count of all words in tweets of that personality type (\(N_{y} + \alpha \cdot N_{\text{total}}\)).

We utilized CountVectorizer to convert text data into feature vectors, focusing on term frequencies in each document. To optimize the model's performance, hyperparameter fine-tuning was implemented, specifically exploring different values for the Laplace smoothing parameter \(\alpha\). Laplace smoothing, crucial for handling the zero probability problem that occurs when the model encounters words it was not trained on, was determined to be most effective with \(\alpha\) set at 0.1.

In our evaluation, the MNB model demonstrated an accuracy of 0.48 in predicting Myers-Briggs personality types based on tweet content. Complementing accuracy, a confusion matrix visually represented the model's performance across various personality types, offering a nuanced understanding of the model's strengths and potential areas for improvement. For example, this matrix revealed that 50\% of all mistakes were confusions of MBTI types that were only a single trait apart. 

\vskip 0.2in

\end{document}